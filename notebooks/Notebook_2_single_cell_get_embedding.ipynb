{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Single Cell Foundation models**\n",
    "\n",
    "\n",
    "\n",
    "**The central dogma in biology**: information flows in one direction:\n",
    "DNA â†’ RNA â†’ Protein\n",
    "\n",
    "DNA is the library â†’ RNA is the photocopy of a recipe â†’ Protein is the dish made from it\n",
    "\n",
    "\n",
    "**RNA sequencing (RNA-seq)** measures RNA levels in a sample  \n",
    "**Single-cell RNA sequencing (scRNA-seq)** measures RNA in each individual cell\n",
    "\n",
    "\n",
    "| Cell ID | Gene A | Gene B | Gene C |\n",
    "|---------|--------|--------|--------|\n",
    "| Cell 1  |   5    |   0    |   3    |\n",
    "| Cell 2  |   1    |   2    |   0    |\n",
    "| Cell 3  |   0    |   0    |   4    |\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "How many genes are encoded in human genome and captured by scRNA-seq?  \n",
    "between 20K and 40k (it depends on the diffintion of gene.protein encoding genes (20K), Non-coding RNA genes (18K).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Geneformer: Single-Cell Foundation Model**\n",
    "\n",
    "Geneformer is a family of single-cell foundation models based on **transformer architecture**, specifically designed to represent **single-cell RNA sequencing (scRNA-seq) data**, where the data is structured as **cell x gene** matrices.\n",
    "\n",
    "\n",
    "### Geneformer\n",
    "Geneformer is a foundation transformer model pretrained on Genecorpus-30M, a corpus comprised of ~30 million single cell transcriptomes from a broad range of human tissues. Each single cellâ€™s transcriptome is presented to the model as a rank value encoding where genes are ranked by their expression in that cell normalized by their expression across the entire Genecorpus-30M. Geneformer displays both zero-shot capabilities as well as fine-tuning tasks relevant to chromatin and network dynamics.\n",
    "- ðŸ“„ [Paper](https://www.nature.com/articles/s41586-023-06139-9): The paper that made it into Nature!\n",
    "\n",
    "\n",
    "### ðŸ§¬ About Helical:\n",
    "Helical provides an open-source framework for and gathers state-of-the-art pre-trained genomics and transciptomics bio foundation models. Still work in progress, but we look forward to interacting with the community to build meaningful things. \n",
    "- [Github](https://github.com/helicalAI/helical-package/issues). On our github in the issue section\n",
    "  \n",
    "---\n",
    "\n",
    "### **Pre-training data Overview**\n",
    "\n",
    "V1 = Genecorpus: 30M cells\n",
    "V2= Genecorpus-103M, Genecorpus-95M (exclude highly mutational cells), cancer (14M)\n",
    "\n",
    "### **Model Design Overview**\n",
    "\n",
    "   - **Pretraining Data**: V1 = Genecorpus: 30M cells\n",
    "                            V2= Genecorpus-103M, Genecorpus-95M (exclude highly mutational cells), cancer (14M)\n",
    "   - **Tokenizer Size**: Adjustable based on the model variant.\n",
    "   - **Input Shape**: The input is represented as `(S, Xi)`, where `S` refers to the number of cells and `Xi` refers to the number of genes (e.g., 2048 or 4096 genes per cell).\n",
    "   - **Embedding Dimensions** (`d_model`): Typically, `256` embedding dimensions, with a **feedforward network size** of `512`.\n",
    "   - **Layers**: Geneformer models range from **6 to 20 layers** of transformer blocks.\n",
    "   - **Encoder/Decoder Architecture**: The model primarily uses an **encoder-only** architecture with multi-head self-attention.\n",
    "   - **Attention Heads**: Uses **4 attention heads** for multi-head self-attention layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tokenizer and Data Processing**\n",
    "\n",
    "Geneformer uses **Ensembl IDs** for tokenization. This enables consistent gene representation and facilitates model interpretability across different gene sets.\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Gene names are mapped into **Ensembl IDs** using the `pyensembl` package.\n",
    "   - Each normalized gene expression vector is converted to **tokenized rank value encoding**, which is a process of transforming the gene expression data into a numerical format that can be fed into the model.\n",
    "\n",
    "2. **Genomic Data Processing**:\n",
    "   - The model processes the genomic data by mapping genes to their corresponding Ensembl IDs. \n",
    "\n",
    "3. **Input to Output Transformation**:\n",
    "   - **Input**: An AnnData object containing the scRNA-seq data is transformed into a Hugging Face **Dataset object**, which is compatible with the model.\n",
    "   - **Output**: The processed data is tokenized and passed through the model for embedding extraction.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Embeddings**:\n",
    "\n",
    "The model processes input data into an embedding tensor of shape `(S, Xi)`, where:\n",
    "   - `S` represents the number of cells.\n",
    "   - `Xi` represents the number of genes per cell (which can be fixed at 2048 or 4096 genes depending on the model).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Attention Mechanism**\n",
    "\n",
    "Geneformer uses a **multi-head self-attention mechanism**, which enables the model to focus on different aspects of the input sequence at different positions. The number of attention heads (e.g., **4** heads) allows the model to capture more complex relationships and dependencies between genes across cells.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Architecture**\n",
    "\n",
    "- **Encoder-Only Architecture**: Geneformer uses an encoder-only architecture, which is ideal for tasks like **embedding generation** and **sequence classification**. This architecture allows the model to learn representations of gene expression sequences effectively.\n",
    "  \n",
    "- **No Decoder**: The model does not use a decoder, making it suitable for tasks such as embedding extraction and sequence-based prediction rather than autoregressive generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications and Outputs**\n",
    "\n",
    "- **Embeddings**: The model is designed to compute different embeddings for each chosen `emb_mode`, depending on the userâ€™s task. For instance, users can obtain:\n",
    "\n",
    "- **Embedding Modes** (`emb_mode`):\n",
    "   - **`cell`**: Returns **mean embeddings** across all tokens in a cell, providing a holistic view of the cellâ€™s gene expression profile.\n",
    "   - **`gene`**: Returns embeddings for each gene token, along with corresponding **Ensembl IDs**. This mode helps focus on individual gene-level information.\n",
    "   - **`cls`**: Returns embeddings of the **CLS token**, which is a special token representing the entire sequence and capturing the global context of the input sequence.\n",
    "\n",
    "  - Global cell embeddings (via the `cell` mode),\n",
    "  - Gene-specific embeddings (via the `gene` mode),\n",
    "  - Sequence-level embeddings (via the `cls` mode).\n",
    "  \n",
    "- **Output**: The output of the model is a **tokenized dataset**, which is processed into a **numpy array** containing the embeddings. These embeddings can then be used for downstream tasks such as:\n",
    "  - **Gene expression analysis**,\n",
    "  - **Cell-type classification**,\n",
    "  - **Clustering**, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### **End-to-End Pipeline**\n",
    "\n",
    "1. **Input Data**: An AnnData object containing single-cell RNA-seq data.\n",
    "You can download annadata objects from https://cellxgene.cziscience.com/datasets\n",
    "scanpy is a python package for processing annadata object (https://scanpy.readthedocs.io/en/stable/index.html)  \n",
    "3. **Data Transformation**: Convert the AnnData object into a Hugging Face dataset object.\n",
    "4. **Embedding Computation**: Use the transformer model to compute the embeddings for the given cells and genes.\n",
    "5. **Application**: Use the embeddings for tasks like gene classification, clustering, and analysis.\n",
    "\n",
    "By leveraging the **Geneformer** architecture, users can analyze large-scale single-cell genomic data, uncovering patterns and relationships within the gene expression profiles of individual cells.\n",
    "\n",
    "---\n",
    "\n",
    "This model offers a robust framework for extracting meaningful insights from scRNA-seq data, enabling high-throughput, scalable analysis in genomics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload scRNA-seq in annadata format\n",
    "\n",
    "![AnnData Schema](../images/anndata_schema.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "\n",
    "ann_data = ad.read_h5ad(\"../yolksac_human.h5ad\")\n",
    "\n",
    "print(ann_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ann_data.obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ann_data.var.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ann_data.var.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(ann_data.X[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helical framework procides thre prepared codes for Geneformer\n",
    "\n",
    "1- prpeare configuration\n",
    "2- run the model\n",
    "3- fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.geneformer import Geneformer, GeneformerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION PARAMETERS\n",
    "\n",
    "## Parameters\n",
    "    ----------\n",
    "###   model_name : Literal[\"gf-6L-30M-i2048\", \"gf-12L-30M-i2048\", \"gf-12L-95M-i4096\", \"gf-20L-95M-i4096\", \"gf-12L-95M-i4096-CLcancer\"], optional, default=\"gf-12L-30M-i2048\"\n",
    "        The name of the model.\n",
    "### batch_size : int, optional, default = 24\n",
    "        The batch size\n",
    "###    emb_layer : int, optional, default = -1\n",
    "        The embedding layer\n",
    "###    emb_mode : Literal[\"cls\", \"cell\", \"gene\"], optional, default=\"cell\"\n",
    "        The embedding mode to use. \"cls\" is only available for Geneformer v2 models, returning the embeddings of the cls token.\n",
    "        For cell level embeddings, a mean across all embeddings excluding the cls token is returned.\n",
    "        For gene level embeddings, each gene token embedding is returned along with the corresponding ensembl ID.\n",
    "###    device : Literal[\"cpu\", \"cuda\"], optional, default=\"cpu\"\n",
    "        The device to use. Either use \"cuda\" or \"cpu\".\n",
    "###    accelerator : bool, optional, default=False\n",
    "        The accelerator configuration. By default same device as model.\n",
    "###    nproc: int, optional, default=1\n",
    "        Number of processes to use for data processing.\n",
    "###    custom_attr_name_dict : dict, optional, default=None\n",
    "        A dictionary that contains the names of the custom attributes to be added to the dataset.\n",
    "        The keys of the dictionary are the names of the custom attributes, and the values are the names of the columns in adata.obs.\n",
    "        For example, if you want to add a custom attribute called \"cell_type\" to the dataset, you would pass custom_attr_name_dict = {\"cell_type\": \"cell_type\"}.\n",
    "        If you do not want to add any custom attributes, you can leave this parameter as None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "model_name = \"gf-12L-95M-i4096\"\n",
    "batch_size = 20\n",
    "emb_layer = -1\n",
    "emb_mode = \"gene\" # or \"cell\", or \"cls\"\n",
    "device = \"cpu\"  # or \"cuda\" for GPU\n",
    "accelerator = True\n",
    "nproc = 4\n",
    "\n",
    "# Example configuration\n",
    "model_config = GeneformerConfig(    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    emb_layer=emb_layer,\n",
    "    emb_mode=emb_mode,\n",
    "    device=device,\n",
    "    accelerator=accelerator,\n",
    "    nproc=nproc\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geneformer class\n",
    "\n",
    "1- process_data\n",
    "2- get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_v2 = Geneformer(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing\n",
    "\n",
    "## Convert each cell into words [ 124,22,33,1,555, .......]\n",
    "\n",
    "1- map gene names to gene ids\n",
    "2- map gene ids to tokenization dictionary\n",
    "3- rank genes\n",
    "4- truncate \n",
    "convert annadata object into a tokenized data in hugging face format\n",
    "## map gene names to ensembl names\n",
    "\n",
    "## Tokenize:\n",
    "    Convert normalized gene expression vector to tokenized rank value encoding.\n",
    "\n",
    "Max input size of model to truncate input to.\n",
    "            | For the 30M model series, should be 2048. For the 95M model series, should be 4096.\n",
    "        special_token : bool = True\n",
    "\n",
    "special_token : bool = True\n",
    "            | Adds CLS token before and EOS token after rank value encoding.\n",
    "            | For the 30M model series, should be False. For the 95M model series, should be True.\n",
    "\n",
    "\n",
    "Normalize expression using gene medians\n",
    "\n",
    "Rank genes by normalized expression per cell\n",
    "\n",
    "Convert genes to tokens using Ensembl ID â†’ Token dictionary\n",
    "\n",
    "Add special tokens (CLS/EOS) if needed for model type\n",
    "\n",
    "Output: Tokenized dataset ready for transformer-based modeling\n",
    "#############################################\n",
    "\n",
    "Ensembl ID\tToken\n",
    "ENSG00000139618\t42\n",
    "ENSG00000227232\t97\n",
    "ENSG00000198786\t185\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?geneformer_v2.process_data\n",
    "\n",
    "dataset = geneformer_v2.process_data(ann_data[:100])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate  embeddings = run the model\n",
    "embeddings = geneformer_v2.get_embeddings(dataset)\n",
    "#print(\"Base model embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"Genes per embedding: {len(embeddings[0])}\")\n",
    "print(f\"Values per gene embedding: {len(embeddings[0].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows are generated in your embeddings?  \n",
    "How many genes?  \n",
    "How many embeddings (numbers, d_model) per gene?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
