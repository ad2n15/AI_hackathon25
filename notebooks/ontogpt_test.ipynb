{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3cc458-0cfc-41e4-8d06-00b8087fe865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=../.local/bin:$PATH\n"
     ]
    }
   ],
   "source": [
    "%env PATH=../.local/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3feec26-685a-4f8f-9d42-c19ada38819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: ontogpt complete [OPTIONS] [INPUT]\n",
      "\n",
      "  Prompt completion.\n",
      "\n",
      "  The input argument may be:     A file path,     or a string. Use the\n",
      "  -i/--input-file option followed by the path to the input file. Otherwise,\n",
      "  the input is assumed to be a string to be read as input.\n",
      "\n",
      "Options:\n",
      "  -i, --inputfile TEXT            Path to a file containing input text.\n",
      "  -m, --model TEXT                Model name to use, e.g. orca-mini-7b or\n",
      "                                  gpt-4. See all model names with ontogpt\n",
      "                                  list-models.\n",
      "  -o, --output FILENAME           Output file.\n",
      "  -O, --output-format [json|yaml|pickle|md|html|owl|turtle|jsonl|kgx|csv|tsv]\n",
      "                                  Output format.\n",
      "  --show-prompt / --no-show-prompt\n",
      "                                  If set, show all prompts passed to model\n",
      "                                  through an API. Use with verbose setting.\n",
      "                                  [default: no-show-prompt]\n",
      "  --api-base TEXT                 Base to use for LLM API, e.g. for the Azure\n",
      "                                  OpenAI API. Note this may also be set\n",
      "                                  through the runoak set-apikey command.\n",
      "  --api-version TEXT              Version to use for LLM API, e.g. for the\n",
      "                                  Azure OpenAI API. Note this may also be set\n",
      "                                  through the runoak set-apikey command.\n",
      "  --model-provider TEXT           Specify a provider if model is not specified\n",
      "                                  in the model name. If using a proxy using\n",
      "                                  the OpenAI API format, this should be set to\n",
      "                                  'openai'.\n",
      "  --system-message TEXT           System message to provide to the LLM, e.g.,\n",
      "                                  'You will extract knowledge from this text.'\n",
      "  -p, --temperature FLOAT         Temperature for model completion.\n",
      "  --cut-input-text / --no-cut-input-text\n",
      "                                  If set, outputs will contain only the first\n",
      "                                  1000 characters of each input text. This is\n",
      "                                  useful when processing many documents.\n",
      "                                  [default: no-cut-input-text]\n",
      "  --api-base TEXT                 Base to use for LLM API, e.g. for the Azure\n",
      "                                  OpenAI API. Note this may also be set\n",
      "                                  through the runoak set-apikey command.\n",
      "  --api-version TEXT              Version to use for LLM API, e.g. for the\n",
      "                                  Azure OpenAI API. Note this may also be set\n",
      "                                  through the runoak set-apikey command.\n",
      "  --model-provider TEXT           Specify a provider if model is not specified\n",
      "                                  in the model name. If using a proxy using\n",
      "                                  the OpenAI API format, this should be set to\n",
      "                                  'openai'.\n",
      "  --system-message TEXT           System message to provide to the LLM, e.g.,\n",
      "                                  'You will extract knowledge from this text.'\n",
      "  --help                          Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!ontogpt complete --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30cc625-98a3-40ff-9029-1336d28aaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get to the other tentacles!\n"
     ]
    }
   ],
   "source": [
    "!ontogpt complete -m ollama/gemma:7b  \"Why did the squid cross the coral reef?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
