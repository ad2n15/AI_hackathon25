{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune HyenaDNA On A Downstream Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will fine-tune the HynaDNA model on a downstream binary classification task involving cis-regulatory elements.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/helicalAI/helical/blob/main/examples/notebooks/HyenaDNA-Fine-Tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What are DNA Features?**\n",
    "DNA features refer to specific regions or sequences within the DNA molecule that serve distinct biological functions.\n",
    "For example, promoters, enhancers, and Transcription factors binding sites\n",
    "\n",
    "These features **regulates** the genetic information and cellular processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HyenaDNA*** \n",
    "Hyena is a genomic foundation model designed to process DNA sequences at single-nucleotide resolution over extremely long contexts—up to 1 million tokens. This represents a 500x increase in context length compared to previous Transformer-based models, which were limited to 512–4,000 tokens due to the quadratic scaling of attention mechanisms.\n",
    "\n",
    " Key Innovations\n",
    "Single-Nucleotide Tokenization: HyenaDNA uses a vocabulary of just five tokens—A, C, G, T, and N—allowing it to model DNA sequences at the finest possible resolution without relying on k-mers or token aggregation \n",
    "Hugging Face\n",
    "\n",
    "Hyena Operator Architecture: Instead of attention, HyenaDNA employs the Hyena operator—a convolution-based mechanism that scales sub-quadratically with sequence length. This enables efficient training and inference on long sequences \n",
    "Hugging Face\n",
    "\n",
    "Sequence Length Warm-Up: To stabilize training on ultra-long sequences, HyenaDNA introduces a scheduling technique that gradually increases sequence length during training. This approach reduces training time by up to 40% and improves accuracy \n",
    "ar5iv\n",
    "\n",
    "In-Context Learning: HyenaDNA demonstrates the first use of in-context learning in genomics, allowing it to adapt to new tasks without updating pretrained model weights. This is achieved through techniques like soft prompting and instruction fine-tuning \n",
    "\n",
    "\n",
    "HyenaDNA's capabilities make it suitable for a range of genomic tasks, including:​\n",
    "\n",
    "Predicting regulatory elements\n",
    "\n",
    "Analyzing chromatin profiles\n",
    "\n",
    "Species classification\n",
    "\n",
    "Identifying enhancers and promoters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig, HyenaDNAFineTuningModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import logging, warnings\n",
    "import numpy as np\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make use of GPU if there is a GPU present, otherwise default to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now use the Hugging Face API to download the [dataset](https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks_revised)\n",
    "- We select the task from the list of tasks available for this dataset (InstaDepp)\n",
    "\n",
    "| Task                  | Number of train sequences | Number of test sequences | Number of labels | Sequence length |\n",
    "| --------------------- | ------------------------- | ------------------------ | ---------------- | --------------- |\n",
    "| promoter_all          | 30,000                    | 1,584                    | 2                | 300             |\n",
    "| promoter_tata         | 5,062                     | 212                      | 2                | 300             |\n",
    "| promoter_no_tata      | 30,000                    | 1,372                    | 2                | 300             |\n",
    "| enhancers             | 30,000                    | 3,000                    | 2                | 400             |\n",
    "| enhancers_types       | 30,000                    | 3,000                    | 3                | 400             |\n",
    "| splice_sites_all      | 30,000                    | 3,000                    | 3                | 600             |\n",
    "| splice_sites_acceptor | 30,000                    | 3,000                    | 2                | 600             |\n",
    "| splice_sites_donor    | 30,000                    | 3,000                    | 2                | 600             |\n",
    "| H2AFZ                 | 30,000                    | 3,000                    | 2                | 1,000           |\n",
    "| H3K27ac               | 30,000                    | 1,616                    | 2                | 1,000           |\n",
    "| H3K27me3              | 30,000                    | 3,000                    | 2                | 1,000           |\n",
    "| H3K36me3              | 30,000                    | 3,000                    | 2                | 1,000           |\n",
    "| H3K4me1               | 30,000                    | 3,000                    | 2                | 1,000           |\n",
    "| H3K4me2               | 30,000                    | 2,138                    | 2                | 1,000           |\n",
    "| H3K4me3               | 30,000                    | 776                      | 2                | 1,000           |\n",
    "| H3K9ac                | 23,274                    | 1,004                    | 2                | 1,000           |\n",
    "| H3K9me3               | 27,438                    | 850                      | 2                | 1,000           |\n",
    "| H4K20me1              | 30,000                    | 2,270                    | 2                | 1,000           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"promoter_tata\"\n",
    "dataset_train = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\", label, split=\"train\", trust_remote_code=True)\n",
    "dataset_test = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\", label, split=\"test\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset_train.shuffle(seed=42)  # you can set a seed for reproducibility\n",
    "dataset_train = shuffled_dataset.select(range(100))\n",
    "\n",
    "unique, counts = np.unique(dataset_train[\"label\"], return_counts=True)\n",
    "dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset_test.shuffle(seed=42)  # you can set a seed for reproducibility\n",
    "dataset_test = shuffled_dataset.select(range(20))\n",
    "\n",
    "unique, counts = np.unique(dataset_test[\"label\"], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HyenaDNA Model Configuration Options**\n",
    "\n",
    "**model_name**\n",
    "Specifies the name or variant of the model to use. Examples include:\n",
    "\n",
    "\"hyenadna-tiny-1k-seqlen\": a lightweight model for short sequences.\n",
    "\n",
    "\"hyenadna-tiny-1k-seqlen-d256\": similar, but with a different embedding dimension.\n",
    "\n",
    "**batch_size**\n",
    "Defines how many sequences are processed together during training or inference. Affects memory usage and training speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary classification task, only 2 labels are present below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our HyenaDNA fine-tuning model and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_config = HyenaDNAConfig(model_name=\"hyenadna-tiny-1k-seqlen-d256\", batch_size=10, device=device)\n",
    "\n",
    "\n",
    "hyena_fine_tune = HyenaDNAFineTuningModel(hyena_config, fine_tuning_head=\"classification\", output_size=len(np.unique(dataset_train[\"label\"])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Parameters\n",
    "hyena_config\n",
    "Type: HyenaDNAConfig\n",
    "This is the configuration object for the base HyenaDNA model. It includes all architectural and training parameters used when instantiating the original HyenaDNA model (e.g., number of layers, embedding size, sequence length, dropout rates, etc.). This config ensures consistency between pretraining and fine-tuning.\n",
    "\n",
    "fine_tuning_head\n",
    "Type: Literal[\"classification\", \"regression\"] | HelicalBaseFineTuningHead\n",
    "Specifies the type of task-specific head to append to the base model for fine-tuning:\n",
    "\n",
    "\"classification\": Adds a classification head suitable for categorical outputs (e.g., predicting labels).\n",
    "\n",
    "\"regression\": Adds a regression head for continuous-valued outputs.\n",
    "\n",
    "Alternatively, a custom head can be provided by subclassing HelicalBaseFineTuningHead for advanced use cases.\n",
    "\n",
    "output_size\n",
    "Type: Optional[int], default = None\n",
    "Defines the number of output units in the fine-tuning head:\n",
    "\n",
    "Required if using a built-in \"classification\" or \"regression\" head.\n",
    "\n",
    "For classification, this typically equals the number of classes.\n",
    "\n",
    "For regression, this is usually 1 (for scalar output) or more for multi-target regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the dataset for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = hyena_fine_tune.process_data(dataset_train[\"sequence\"])\n",
    "test_dataset = hyena_fine_tune.process_data(dataset_test[\"sequence\"])\n",
    "\n",
    "train_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we set our training and validation datasets along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_fine_tune.train(train_dataset=train_dataset, train_labels=dataset_train[\"label\"], validation_dataset=test_dataset, validation_labels=dataset_test[\"label\"], epochs=2, optimizer_params={\"lr\": 2e-6}, lr_scheduler_params={\"name\": \"linear\", \"num_warmup_steps\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the outputs of the model on our test dataset and display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = hyena_fine_tune.get_outputs(test_dataset)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(dataset_test[\"label\"], outputs.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(dataset_test[\"label\"], outputs.argmax(axis=1))\n",
    "\n",
    "# Perform row-wise normalization\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Get unique labels in the order they appear in the confusion matrix\n",
    "unique_labels = np.unique(np.concatenate((dataset_test[\"label\"], outputs.argmax(axis=1))))\n",
    "\n",
    "# Create and plot the normalized confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=unique_labels)\n",
    "disp.plot(ax=ax, xticks_rotation='vertical', values_format='.2f', cmap='coolwarm')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Normalized Confusion Matrix (Row-wise)')\n",
    "fig.set_facecolor(\"none\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
